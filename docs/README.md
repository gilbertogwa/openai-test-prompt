### ğŸ’¡ **Exemplos PrÃ¡ticos de ConfiguraÃ§Ã£o:**

#### ğŸƒâ€â™‚ï¸ **Modo RÃ¡pido (muitos testes):**
```bash
# .env
PARALLEL_TESTS=10
LOG_LEVEL=error
LOG_TOKENS=false
GENERATE_HTML_REPORT=false
```

#### ğŸ” **Modo Debug (investigar problemas):**
```bash
# .env  
PARALLEL_TESTS=1
LOG_LEVEL=debug
VERBOSE_MODE=true
RETRY_ATTEMPTS=1
```

#### ğŸ“Š **Modo ProduÃ§Ã£o (relatÃ³rios completos):**
```bash
# .env
PARALLEL_TESTS=3
GENERATE_HTML_REPORT=true
SAVE_JSON_REPORT=true
LOG_TOKENS=true
```

### Timeout e Retry por Teste
VocÃª tambÃ©m pode configurar no arquivo de teste individual:

```json
{
  "config": {
    "timeout": 30000,     // 30 segundos
    "retries": 3,         // 3 tentativas
    "skipOnError": false  // Parar se houver erro
  },
  "tests": [...]
}
```# ğŸ¤– Framework de Testes Automatizados para LLM

Um sistema simples e poderoso para testar APIs de modelos de linguagem, projetado para ser usado por **qualquer pessoa**, mesmo sem conhecimento tÃ©cnico em programaÃ§Ã£o.

## ğŸ¯ Objetivo

Este framework permite que vocÃª:
- âœ… Teste automaticamente respostas de APIs de LLM
- âœ… Compare resultados esperados vs obtidos
- âœ… Monitore consumo de tokens
- âœ… Gere relatÃ³rios detalhados
- âœ… Crie novos testes apenas editando arquivos JSON

## ğŸš€ InÃ­cio RÃ¡pido

### 1. InstalaÃ§Ã£o

```bash
# Clone o projeto
git clone https://github.com/seu-usuario/llm-automated-tests
cd llm-automated-tests

# Instale as dependÃªncias
npm install

# Configure o Playwright
npm run setup
```

### 2. ConfiguraÃ§Ã£o

#### 2.1 Configure sua API
Edite o arquivo `config/api.conf` (configuraÃ§Ãµes da API):

```ini
API_URL=https://api.openai.com/v1/chat/completions
BEARER_TOKEN=sk-seu-token-aqui
```

#### 2.2 Configure a ExecuÃ§Ã£o dos Testes (Opcional)
Copie e edite o arquivo `.env` (configuraÃ§Ãµes de execuÃ§Ã£o):

```bash
cp .env.example .env
```

Edite `.env` conforme necessÃ¡rio:
```bash
# ConfiguraÃ§Ãµes de performance
PARALLEL_TESTS=3
TIMEOUT=30000
RETRY_ATTEMPTS=3

# ConfiguraÃ§Ãµes de relatÃ³rios
GENERATE_HTML_REPORT=true
LOG_TOKENS=true
```

#### 2.3 Personalize o Prompt (Opcional)
Edite `config/prompt.md` com as instruÃ§Ãµes especÃ­ficas para seu modelo.

### 3. Execute um Teste de Exemplo

```bash
npm run test
```

## ğŸ“ Estrutura do Projeto

```
projeto/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ body.json        # â† Template da requisiÃ§Ã£o  
â”‚   â””â”€â”€ prompt.md        # â† InstruÃ§Ã£o para o modelo
â”œâ”€â”€ .env                 # â† Configure EXECUÃ‡ÃƒO aqui (opcional)
â”œâ”€â”€ .env.example         # â† Template das configuraÃ§Ãµes de execuÃ§Ã£o
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ exemplo-basico.json    # â† Seus testes ficam aqui
â”‚   â””â”€â”€ exemplo-avancado.json
â”œâ”€â”€ reports/             # â† RelatÃ³rios sÃ£o salvos aqui
â””â”€â”€ README.md           # â† Este arquivo
```

### ğŸ”§ **ConfiguraÃ§Ãµes Separadas:**

| Arquivo | Responsabilidade | ObrigatÃ³rio |
|---------|------------------|-------------|
| `.env` | âš™ï¸ **ExecuÃ§Ã£o**: Timeout, Paralelos, RelatÃ³rios | âŒ NÃ£o |

## ğŸ“ Como Criar Seus PrÃ³prios Testes

### Passo 1: Copie um Exemplo
```bash
cp tests/exemplo-basico.json tests/meus-testes.json
```

### Passo 2: Edite o Arquivo
Abra `tests/meus-testes.json` em qualquer editor de texto:

```json
{
  "metadata": {
    "name": "Meus Testes Personalizados",
    "description": "DescriÃ§Ã£o do que estes testes validam"
  },
  "tests": [
    {
      "id": "meu-teste-001",
      "name": "Nome descritivo do teste",
      "entrada": "Texto que serÃ¡ enviado para o modelo",
      "saida": {
        "formato": "esperado",
        "da": "resposta"
      }
    }
  ]
}
```

### Passo 3: Execute Seus Testes
```bash
npm run test
```

## ğŸ¯ Exemplos PrÃ¡ticos

### Exemplo 1: Teste Simples
```json
{
  "tests": [
    {
      "name": "Teste de saudaÃ§Ã£o",
      "entrada": "Diga olÃ¡ em portuguÃªs",
      "saida": {
        "mensagem": "OlÃ¡!"
      }
    }
  ]
}
```

### Exemplo 2: Teste com MÃºltiplos Campos
```json
{
  "tests": [
    {
      "name": "AnÃ¡lise de sentimento",
      "entrada": "Estou muito feliz hoje!",
      "saida": {
        "sentimento": "positivo",
        "confianca": 0.95,
        "palavras_chave": ["feliz", "hoje"]
      }
    }
  ]
}
```

### Exemplo 3: Teste de Lista
```json
{
  "tests": [
    {
      "name": "ExtraÃ§Ã£o de cidades",
      "entrada": "Visitei SÃ£o Paulo e Rio de Janeiro",
      "saida": {
        "cidades": [
          {"nome": "SÃ£o Paulo", "uf": "SP"},
          {"nome": "Rio de Janeiro", "uf": "RJ"}
        ]
      }
    }
  ]
}
```

## ğŸ› ï¸ Comandos DisponÃ­veis

### Executar Testes
```bash
# Todos os testes (usa configuraÃ§Ãµes do .env)
npm run test

# Arquivo especÃ­fico
npm run test:file tests/meu-arquivo.json

# Com tags especÃ­ficas
npm run test:tags basico,importante

# Sobrescrever configuraÃ§Ãµes temporariamente
PARALLEL_TESTS=1 LOG_LEVEL=debug npm run test
GENERATE_HTML_REPORT=true npm run test
```

### Validar Arquivos
```bash
# Validar sintaxe de um arquivo
npm run validate tests/meu-teste.json

# Validar todos os arquivos
npm run validate
```

### Gerar Novos Testes
```bash
# Criar template de teste
npm run create:test --name="Meus Novos Testes"
```

## ğŸ“Š Entendendo os Resultados

### SaÃ­da no Console
```
ğŸ§ª Executando Teste 1: Teste de saudaÃ§Ã£o
ğŸ“ Entrada: "Diga olÃ¡ em portuguÃªs"
ğŸª™ Tokens: 150 (prompt: 100, completion: 50)
â±ï¸  Tempo: 1.2s
âœ… Teste PASSOU

ğŸ“Š RESUMO DOS TESTES
âœ… Sucessos: 3 (75%)
âŒ Falhas: 1 (25%)  
ğŸª™ Total de tokens: 450
â±ï¸  Tempo total: 4.5s
```

### RelatÃ³rio JSON
Um arquivo detalhado Ã© salvo em `reports/test-results.json` com:
- Resultados completos de cada teste
- EstatÃ­sticas de tokens
- Tempo de execuÃ§Ã£o
- Respostas completas da API

## âš™ï¸ ConfiguraÃ§Ãµes

```bash
# Apenas configuraÃ§Ãµes relacionadas Ã  API
API_URL=https://api.openai.com/v1/chat/completions
BEARER_TOKEN=sk-seu-token-aqui

# Performance & Timeout
PARALLEL_TESTS=5          # Quantos testes executar simultÃ¢neos
TIMEOUT=30000             # Timeout por requisiÃ§Ã£o (ms)
RETRY_ATTEMPTS=3          # Tentativas em caso de falha
REQUEST_DELAY=500         # Pausa entre requisiÃ§Ãµes (ms)

# Logging & Debug
LOG_LEVEL=info            # debug, info, warn, error
LOG_TOKENS=true           # Mostrar consumo de tokens
VERBOSE_MODE=false        # Logs detalhados

# RelatÃ³rios
GENERATE_HTML_REPORT=true # RelatÃ³rio visual
SAVE_JSON_REPORT=true     # Dados em JSON
REPORTS_DIR=reports       # Pasta dos relatÃ³rios
```

### Tags para OrganizaÃ§Ã£o
Use tags para organizar seus testes:

```json
{
  "tests": [
    {
      "name": "Teste importante",
      "entrada": "...",
      "saida": {...},
      "tags": ["critico", "regressao", "api-v2"]
    }
  ]
}
```

Execute apenas testes com tags especÃ­ficas:
```bash
npm run test:tags critico,regressao
```

## ğŸ”§ Troubleshooting

### Erro: ".env nÃ£o encontrado"
1. Crie o arquivo `.env`
2. Adicione pelo menos:
   ```ini
   API_URL=https://api.openai.com/v1/chat/completions
   BEARER_TOKEN=sk-seu-token
   ```

### Erro: "API Token invÃ¡lido"
1. Verifique se o token em `.env` estÃ¡ correto
2. Confirme se o token nÃ£o expirou
3. Teste o token em uma ferramenta como Postman

### ConfiguraÃ§Ãµes nÃ£o estÃ£o funcionando
1. Arquivo `.env` estÃ¡ na raiz do projeto?
2. VariÃ¡veis estÃ£o no formato: `NOME=valor` (sem espaÃ§os)?
3. Reinicie o teste apÃ³s alterar configuraÃ§Ãµes

### Performance ruim ou timeouts
```bash
# No arquivo .env:
PARALLEL_TESTS=1       # Reduzir paralelismo
TIMEOUT=60000         # Aumentar timeout
REQUEST_DELAY=1000    # Aumentar pausa entre requests
```

### Erro: "JSON invÃ¡lido no arquivo de teste"
1. Use um validador JSON online para verificar a sintaxe
2. Certifique-se de que todas as vÃ­rgulas e chaves estÃ£o corretas
3. Execute: `npm run validate tests/seu-arquivo.json`

### Teste sempre falha
1. Verifique se o formato da `saida` corresponde exatamente ao retorno da API
2. Use o modo debug: 
   ```bash
   # No .env:
   LOG_LEVEL=debug
   VERBOSE_MODE=true
   ```
3. Compare o JSON esperado vs obtido no relatÃ³rio

## ğŸ“ Dicas para UsuÃ¡rios NÃ£o-TÃ©cnicos

### 1. Comece Simples
- Copie um exemplo existente
- Mude apenas a `entrada` e `saida`
- Teste com um arquivo por vez

### 2. Use um Editor de Texto AmigÃ¡vel
- **Windows**: Notepad++, VSCode
- **Mac**: TextEdit (modo texto simples), VSCode  
- **Online**: JSON Editor Online

### 3. Valide Sempre
Antes de executar, valide seu arquivo:
```bash
npm run validate tests/meu-arquivo.json
```

### 4. Teste Incrementalmente
- Comece com 1-2 testes
- VÃ¡ adicionando gradualmente
- Sempre execute apÃ³s mudanÃ§as

### 5. Use Nomes Descritivos
```json
{
  "name": "Teste extraÃ§Ã£o de email de texto longo",
  "entrada": "Meu email Ã© joao@empresa.com...",
  "saida": {"email": "joao@empresa.com"}
}
```

## ğŸ“ˆ Monitoramento de Custos

O framework monitora automaticamente:
- **Tokens de prompt**: Custo da instruÃ§Ã£o
- **Tokens de completion**: Custo da resposta  
- **Total de tokens**: Custo total

### Estimativa de Custos (OpenAI GPT-4o-mini)
- Input: $0.15 / 1M tokens
- Output: $0.60 / 1M tokens

Exemplo: 1000 tokens = ~$0.0015

## ğŸ”’ SeguranÃ§a

### ProteÃ§Ã£o de Credenciais
- âŒ Nunca commite tokens no Git
- âœ… Use o arquivo `.env`
- âœ… Configure `.gitignore` adequadamente

### Dados SensÃ­veis
- âŒ NÃ£o inclua dados pessoais nos testes
- âœ… Use dados fictÃ­cios ou anonimizados
- âœ… Revise logs antes de compartilhar

## ğŸ¤ Suporte

### Problemas Comuns
Consulte a seÃ§Ã£o **Troubleshooting** acima.

### Precisa de Ajuda?
1. Verifique os exemplos em `tests/`
2. Execute `npm run validate` para verificar erros
3. Consulte os logs em `reports/`

### ContribuiÃ§Ã£o
1. Fork do projeto
2. Crie sua branch: `git checkout -b minha-feature`
3. Commit: `git commit -m 'Adiciona nova feature'`
4. Push: `git push origin minha-feature`
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

MIT License - Veja o arquivo LICENSE para detalhes.

---

**ğŸ‰ Pronto! Agora vocÃª pode criar testes automatizados para qualquer API de LLM de forma simples e eficiente!**